Voici votre **README.md** complet avec toutes les instructions :  

```md
# ğŸš— Voiture AI Backend

## ğŸ“Œ PrÃ©requis

Avant de commencer, assurez-vous d'avoir installÃ© :  
- **Python 3.x**  
- **Pip 25.0 ou supÃ©rieur**  
- **Git**  
- **Ollama** (disponible sur [ollama.com](https://ollama.com))

---

## âš¡ Installation

### 1ï¸âƒ£ **Cloner le DÃ©pÃ´t**
```sh
git clone https://github.com/AK-Issac/Backend-AITutor
cd Backend-AITutor
```

---

### 2ï¸âƒ£ **CrÃ©er un Environnement Virtuel**

#### âœ… Windows :
```sh
python -m venv venv
```

#### âœ… Linux/Mac :
```sh
python3 -m venv venv
```

---

### 3ï¸âƒ£ **Activer l'Environnement Virtuel**

#### âœ… Windows :
```sh
venv\Scripts\activate
```

#### âœ… Linux/Mac :
```sh
source venv/bin/activate
```

---

### 4ï¸âƒ£ **Installer les DÃ©pendances**
```sh
pip install -r requirements.txt
```

---

## ğŸš€ Installation et Configuration d'Ollama

### 5ï¸âƒ£ **Installer Ollama**
TÃ©lÃ©chargez et installez Ollama depuis [ollama.com](https://ollama.com).  

Ensuite, ouvrez votre terminal et exÃ©cutez :  
```sh
ollama --version
```
Si la commande fonctionne, Ollama est bien installÃ©.

---

### 6ï¸âƒ£ **TÃ©lÃ©charger le modÃ¨le LLaMA**
ExÃ©cutez cette commande pour tÃ©lÃ©charger **LLaMA 3.2** :  
```sh
ollama pull llama3.2
```
Vous pouvez vÃ©rifier que le modÃ¨le est bien installÃ© avec :  
```sh
ollama list
```

---

### 7ï¸âƒ£ **Tester Ollama en ligne de commande**
Avant d'intÃ©grer Ollama dans Flask, testons-le directement :  
```sh
ollama run llama3.2
```
Si la commande affiche une rÃ©ponse gÃ©nÃ©rÃ©e par l'IA, tout est bon ! ğŸš€

---

## ğŸ—ï¸ Lancer le Serveur Flask

### 8ï¸âƒ£ **DÃ©marrer Ollama en mode serveur**
Ollama doit Ãªtre actif avant de lancer Flask. Ouvrez un terminal et exÃ©cutez :  
```sh
ollama serve
```

---

### 9ï¸âƒ£ **Lancer l'API Flask**
Dans un autre terminal, dÃ©marrez l'API Flask avec :  
```sh
python main.py
```
Votre serveur sera accessible sur `http://127.0.0.1:8000`.

---

## ğŸ› ï¸ Tester avec Postman

1. **Ouvrir Postman**  
2. **SÃ©lectionner "POST"**  
3. **Entrer l'URL suivante** :  
   ```
   http://127.0.0.1:8000/api/chat
   ```
4. **Aller dans l'onglet "Body" â†’ SÃ©lectionner "raw" â†’ Choisir "JSON"**  
5. **Entrer ce JSON comme requÃªte** :  
   ```json
   {
       "message": "Bonjour, comment vas-tu ?"
   }
   ```
6. **Cliquer sur "Send"** ğŸš€  

Si tout fonctionne, vous recevrez une rÃ©ponse de LLaMA comme ceci :  
```json
{
    "response": "Bonjour ! Je vais bien, merci de demander. Comment puis-je vous aider ?"
}
```

---

## ğŸ” DÃ©pannage

Si vous avez une erreur, vÃ©rifiez les points suivants :

âœ… **Ollama est-il bien installÃ© ?**  
```sh
ollama --version
```

âœ… **Le modÃ¨le est-il bien tÃ©lÃ©chargÃ© ?**  
```sh
ollama list
```

âœ… **Ollama est-il en cours d'exÃ©cution ?**  
```sh
ollama serve
```

âœ… **Le serveur Flask est-il lancÃ© ?**  
```sh
python main.py
```

---

## ğŸš« DÃ©sactiver l'Environnement Virtuel
Si vous souhaitez quitter l'environnement virtuel, exÃ©cutez :  
```sh
deactivate
```

---

ğŸ‰ **FÃ©licitations !** Votre API Flask avec LLaMA fonctionne parfaitement ! ğŸš€
```

Tout est bien structurÃ© et prÃªt Ã  Ãªtre utilisÃ© sur **GitHub** ! ğŸ¯